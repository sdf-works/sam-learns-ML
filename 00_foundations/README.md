# 00 – Foundations

Welcome to the starting point of `sam-learns-ML` — where I (re)build machine learning intuition from the ground up.

This section is about **graceful rigor**. I’m not just trying to get results; I want to understand why they work. 
These early experiments are simple on purpose. They’re here to build muscle memory, not show off.

## Goals

- **Internalize the fundamentals**: Understand loss functions, gradient descent, and the mechanics behind basic ML.
- **Learn by doing**: Every concept will be paired with hands-on code — no copy/pasting, no black boxes.
- **Build good habits**: Clean visualizations, readable code, meaningful experiment logs.
- **Fail forward**: Embrace debugging as part of learning. This is where I mess up, and give the mistake meaning by learning.

## Contents (tentative)

- `loss-curves/`: What does a “good” or “bad” loss curve really look like? Explore overfitting, underfitting, and convergence.
- `linear-regression/`: Fit a line. Do it badly, then do it better. Understand what the loss is telling you.
- `gradient-descent/`: Code it from scratch. Step through each update. See what changes — and what doesn’t.

---

I’m building this public lab notebook to prove that I know what I’m doing — even if it’s not obvious from my resume.
If you’re hiring, collaborating, or just curious: welcome aboard.

