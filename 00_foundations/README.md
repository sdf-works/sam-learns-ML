# 00 â€“ Foundations

Welcome to the starting point of `sam-learns-ML` â€” where I (re)build machine learning intuition from the ground up.

This section is about **graceful rigor**. Iâ€™m not just trying to get results; I want to understand why they work. 
These early experiments are simple on purpose. Theyâ€™re here to build muscle memory, not show off.

## Goals

- **Internalize the fundamentals**: Understand loss functions, gradient descent, and the mechanics behind basic ML.
- **Learn by doing**: Every concept will be paired with hands-on code â€” no copy/pasting, no black boxes.
- **Build good habits**: Clean visualizations, readable code, meaningful experiment logs.
- **Fail forward**: Embrace debugging as part of learning. This is where I mess up, and give the mistake meaning by learning.

## Contents (tentative)

## ğŸ§ª Notebooks Created Today

### `03_loss_curves.ipynb`
**Goal:** Train a neural network on noisy data and visualize how it learns.

- Simulated data using `y = 3x + 2 + noise`
- Built a linear regression model in PyTorch (`nn.Linear`)
- Tracked loss over time using Mean Squared Error (MSE)
- Explored the effects of:
  - Learning rate
  - Weight and bias initialization
  - Training loop and gradient updates
- Visualized predictions vs truth
- Reflected on under/overfitting behavior
- Wrapped up with markdown Q&A and math explanation

ğŸ’¡ *Result:* A great baseline for understanding how even a simple model learns from data.

---

### `04_loss_curves_wiggles.ipynb`
**Goal:** Fit a nonlinear, â€œwigglyâ€ function using deeper neural networks.

- Target function: `y = 3x + 2 + sin(5x) + noise`
- Added hidden layers with activations (`Tanh`, `ReLU`)
- Compared architectures: shallow, wide, and deep ("lasagna layers")
- Experimented with optimizers: `SGD` vs `Adam`
- Took model snapshots at key epochs for side-by-side plots
- Discussed slope misalignment, generalization, and learned curve shapes

ğŸ§  *Highlight:* Switching to `Adam` with a `Tanh` lasagna layer gave the best performance on the noisy nonlinear signal.

---

Let me know if you'd like this written more professionally or with some spice (e.g. pirate-coded metaphors or scientific tone)!

---

Iâ€™m building this public lab notebook to prove that I know what Iâ€™m doing â€” even if itâ€™s not obvious from my resume.
If youâ€™re hiring, collaborating, or just curious: welcome aboard.

