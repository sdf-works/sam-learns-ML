# 🧭 Alignment & Weirdness – Teaching the Robots Not to Lie (or Panic)

This section is for experiments at the intersection of model behavior, safety, and interpretability.

I’m exploring:
- Output probing and behavioral quirks
- Confidence, calibration, and uncertainty
- Jailbreak attempts and edge-case prompts
- What it means for a model to “know” something — and how to tell when it doesn’t

The goal isn’t to just build smarter models — it’s to build models that are a little less weird *in the ways that matter*.

Alignment isn’t just technical — it’s philosophical, behavioral, and deeply strange. That’s why I’m here.
