# ğŸ§­ Alignment & Weirdness â€“ Teaching the Robots Not to Lie (or Panic)

This section is for experiments at the intersection of model behavior, safety, and interpretability.

Iâ€™m exploring:
- Output probing and behavioral quirks
- Confidence, calibration, and uncertainty
- Jailbreak attempts and edge-case prompts
- What it means for a model to â€œknowâ€ something â€” and how to tell when it doesnâ€™t

The goal isnâ€™t to just build smarter models â€” itâ€™s to build models that are a little less weird *in the ways that matter*.

Alignment isnâ€™t just technical â€” itâ€™s philosophical, behavioral, and deeply strange. Thatâ€™s why Iâ€™m here.
