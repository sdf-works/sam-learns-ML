# ðŸ¤– Transformers from Scratch â€“ How the Robots Get in Disguise

This section is my hands-on journey into transformer architecture â€” building, breaking, and understanding the core pieces that power modern language models.

Topics Iâ€™ll cover:
- Self-attention and why itâ€™s weirdly powerful
- Positional encoding (because order still matters)
- Layer normalization, residuals, and other glue pieces
- Building a tiny transformer from scratch
- Interpretability experiments, because I want to *see* what itâ€™s doing

Transformers might be everywhere now, but I want to understand them â€” no magic allowed.
